{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "## Predict Yelp star ratings from review text\n",
    "\n",
    "Unstructured data makes up the vast majority of data. This is a basic intro to handling unstructured data. The objective is to be able to extract the sentiment (positive or negative) from text. We will do this from Yelp reviews.\n",
    "\n",
    "The model will be assessed based on how root mean squared error of the number of stars it predicts.\n",
    "\n",
    "Download the data here : http://thedataincubator.s3.amazonaws.com/coursedata/mldata/yelp_train_academic_dataset_review.json.gz\n",
    "\n",
    "### Load and parse the data\n",
    "Load and parse the json using the `simplejson` library. Then convert it to a `pandas` data frame and remove columns I won't need to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1012913 entries, 0 to 1012912\n",
      "Data columns (total 8 columns):\n",
      "business_id    1012913 non-null object\n",
      "date           1012913 non-null object\n",
      "review_id      1012913 non-null object\n",
      "stars          1012913 non-null int64\n",
      "text           1012913 non-null object\n",
      "type           1012913 non-null object\n",
      "user_id        1012913 non-null object\n",
      "votes          1012913 non-null object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 69.6+ MB\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import gzip\n",
    "import nltk\n",
    "import timeit\n",
    "import simplejson\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import nltk.tokenize as tokenize\n",
    "from sklearn import linear_model\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import cross_validation, grid_search\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns=25\n",
    "\n",
    "data = gzip.open('yelp_train_academic_dataset_review.json.gz')\n",
    "data_content = data.read()\n",
    "data.close()\n",
    "lines= re.split('\\n',data_content)\n",
    "json_data = [simplejson.loads(line) for line in lines[:-1]]\n",
    "df = pd.DataFrame(json_data)\n",
    "del(json_data)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1012913 entries, 0 to 1012912\n",
      "Data columns (total 3 columns):\n",
      "business_id    1012913 non-null object\n",
      "stars          1012913 non-null int64\n",
      "text           1012913 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 30.9+ MB\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop = ['date','review_id','type','user_id','votes']\n",
    "df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "I will first build a bag of words model. I will build a linear model based on the count of the words in each review.\n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "I first need to tokenize the yelp reviews to break them into individual words. For this process I used the `nltk` package stopwords corpus and the `HashingVectorizer` from sklearn. Stop words are words we want to omit because they are common and do not tend to give us any hints as to the meaning of reviews. Example of stop words are: \"a\", \"an\", \"the\", \"it\", \"he\", \"her\". `HashingVectorizer` uses a hashtable to more efficiently store the large dictionary from the yelp reviews. For testing (and scoring against a reference) I am deploying all of my models to heroku, so I essentially wanted to keep things as small as possible. Without this limitation a `CountVectorizer` could employed.\n",
    "\n",
    "An additional limitation of the `HashingVectorizer` is that is does not record the mapping: if you want to go back and forth that information is simply lost. But here I am concerned with a BagOfWords model to predict rating stars. I don't care what words predict 5 stars: `HashingVectorizer` should be perfectly good (and small!)\n",
    "\n",
    "Common english language words were filtered using a nltk subpackage and the stopwords command in HashingVectorizer. This removes words like \"the\", \"and\", etc. which shouldn't have any particular relevence to good or bad reviews.\n",
    "\n",
    "#### Model Fitting\n",
    "\n",
    "I use a `RidgeCV` linear regression to fit my bag of words model to the number of review stars on Yelp. Cross validation was used to validate the test set and prevent overfitting.\n",
    "\n",
    "The `Pipeline` function of sklearn was used to quickly and easily link preprocessing and estimator steps of my machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows = np.random.choice(df.index.values,33000,replace=False)\n",
    "sampled_df = df.ix[rows]\n",
    "sampled_df.to_json('Yelp_review_text_stars_33000.json')\n",
    "del(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hv = HashingVectorizer(norm='l2',stop_words=nltk.corpus.stopwords.words('english'))\n",
    "hvcounts = hv.fit_transform(sampled_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hvcounts.pkl',\n",
       " 'hvcounts.pkl_01.npy',\n",
       " 'hvcounts.pkl_02.npy',\n",
       " 'hvcounts.pkl_03.npy']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#joblib.dump(hv,'hash_vectorizer.pkl') Saved hv from full data\n",
    "#joblib.dump(hvcounts,'hvcounts.pkl') Saved hvcounts from full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_train,text_test,stars_train,stars_test = cross_validation.train_test_split(sampled_df['text'],sampled_df['stars'],\n",
    "                                                                               test_size=0.2, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monogram score:  0.565108558564\n",
      "154.448280271\n"
     ]
    }
   ],
   "source": [
    "tic=timeit.default_timer()\n",
    "\n",
    "monogram_pipeline = Pipeline([\n",
    "('vect', HashingVectorizer(norm='l2',ngram_range=(1,1),stop_words=nltk.corpus.stopwords.words('english'))),\n",
    "('lm', linear_model.SGDRegressor(n_iter=6000,alpha=.00001,penalty='l2')),\n",
    "])\n",
    "\n",
    "monogram_pipeline.fit(text_train,stars_train)\n",
    "score = monogram_pipeline.score(text_test,stars_test)\n",
    "print 'Monogram score: ', score\n",
    "\n",
    "toc=timeit.default_timer()\n",
    "print toc - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram score:  0.413075315485\n",
      "213.984861767\n"
     ]
    }
   ],
   "source": [
    "tic=timeit.default_timer()\n",
    "\n",
    "bigram_pipeline = Pipeline([\n",
    "('vect', HashingVectorizer(norm='l2',ngram_range=(2,2),stop_words=nltk.corpus.stopwords.words('english'))),\n",
    "('lm', linear_model.SGDRegressor(n_iter=6000,alpha=.00001,penalty='l2')),\n",
    "])\n",
    "\n",
    "bigram_pipeline.fit(text_train,stars_train)\n",
    "score = bigram_pipeline.score(text_test,stars_test)\n",
    "print 'Bigram score: ', score\n",
    "\n",
    "toc=timeit.default_timer()\n",
    "print toc - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi- and Monogram Linear Regression score:  0.445895681409\n",
      "37.2129645705\n"
     ]
    }
   ],
   "source": [
    "text_train,text_test,stars_train,stars_test = cross_validation.train_test_split(sampled_df['text'],sampled_df['stars'],\n",
    "                                                                               test_size=0.2, random_state=23)\n",
    "tic=timeit.default_timer()\n",
    "\n",
    "pred_mono_train = monogram_pipeline.predict(text_train)\n",
    "pred_bi_train = bigram_pipeline.predict(text_train)\n",
    "pred_combined_train = [[p1,p2] for p1,p2 in zip(pred_mono_train,pred_bi_train)]\n",
    "lm = linear_model.LinearRegression(normalize=True,n_jobs=2)\n",
    "lm.fit(pred_combined_train,stars_train)\n",
    "\n",
    "pred_mono_test = monogram_pipeline.predict(text_test)\n",
    "pred_bi_test = bigram_pipeline.predict(text_test)\n",
    "pred_combined_test = [[p1,p2] for p1,p2 in zip(pred_mono_test,pred_bi_test)]\n",
    "score = lm.score(pred_combined_test,stars_test)\n",
    "print 'Bi- and Monogram Linear Regression score: ', score\n",
    "\n",
    "toc=timeit.default_timer()\n",
    "print toc - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi- and Monogram score:  0.581969293554\n",
      "314.762666724\n"
     ]
    }
   ],
   "source": [
    "tic=timeit.default_timer()\n",
    "\n",
    "monobi_pipeline = Pipeline([\n",
    "('vect', HashingVectorizer(ngram_range=(1,2),norm='l2',stop_words=nltk.corpus.stopwords.words('english'))),\n",
    "('lm', linear_model.SGDRegressor(n_iter=6000,penalty='l2',alpha=.00001)),\n",
    "])\n",
    "monobi_pipeline.fit(text_train,stars_train)\n",
    "score = monobi_pipeline.score(text_test,stars_test)\n",
    "print 'Bi- and Monogram score: ', score\n",
    "\n",
    "toc=timeit.default_timer()\n",
    "print toc - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi- and Monogram score:  0.580045386659\n",
      "23.8673929312\n"
     ]
    }
   ],
   "source": [
    "tic=timeit.default_timer()\n",
    "\n",
    "monobi_pipeline = Pipeline([\n",
    "('vect', HashingVectorizer(ngram_range=(1,2),norm='l2',stop_words=nltk.corpus.stopwords.words('english'))),\n",
    "('lm', linear_model.Ridge()),\n",
    "])\n",
    "monobi_pipeline.fit(text_train,stars_train)\n",
    "score = monobi_pipeline.score(text_test,stars_test)\n",
    "print 'Bi- and Monogram score: ', score\n",
    "\n",
    "toc=timeit.default_timer()\n",
    "print toc - tic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple linear regression combining a model trained on monograms and a model trained on bigrams has a stronger predictive power than either alone!\n",
    "\n",
    "This combined model beat the benchmark for a bigram model on the Heroku app.\n",
    "\n",
    "### Top restaurant bigrams\n",
    "\n",
    "Looking at only reviews of restaurants I next wanted to identify word pairs in reviews that are more likely than the individual words alone. These might be strongly indicative of \"foodie\" type words that you might expect to find in a yelp review such as \"huevos rancheros\".\n",
    "\n",
    "We can find word pairs that are unlikely to occur consecutively based on the underlying probability of their words.\n",
    "\n",
    "Mathematically, if $p(w)$ be the probability of a word w and $p(w1w2)$ is the probability of the bigram $w1w2$, then we want to look at word pairs $w1w2$ where the statistic\n",
    "\n",
    "$p(w1w2)/p(w1)/p(w2)$\n",
    "\n",
    "is high.\n",
    "\n",
    "This metric is, however, problematic when $p(w_1)$ and/or $p(w_2)$ are small. This can be fixed with Bayesian smoothing or additive smoothing which essentially adds a constant factor to all probabilities. This factor sets the scale for the number of appearances a word must be used in the overall corpus before it is considered relevent.\n",
    "\n",
    "First I need to load in a second data set that idenitifies which businesses are restaurants and do an SQL style join on my two pandas dataframes. This will allow me to select reviews that only correspond to restaurants and by extension special food bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = gzip.open('../ML_Yelp_BusFeatures/yelp_train_academic_dataset_business.json.gz')\n",
    "data_content_biz = data.read()\n",
    "data.close()\n",
    "lines= re.split('\\n',data_content_biz)\n",
    "json_data = [simplejson.loads(line) for line in lines[:-1]]\n",
    "dfbiz = pd.DataFrame(json_data)\n",
    "\n",
    "restaurant = []\n",
    "for i in dfbiz.index:\n",
    "    restaurant.append(sum([1 for cat in dfbiz.iloc[i]['categories'] if re.match('Restaurants',cat)]))\n",
    "dfbiz['restaurant'] = restaurant\n",
    "df_big = pd.merge(df,dfbiz,on='business_id')\n",
    "df_rest = df_big[df_big['restaurant']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also found it necessary at this stage to consider word lemmatization. Lemmatization is an NLP strategy to lower the vocabulary space by combining words that have the same root. For example, lemmatization should catch the plural form of a word and remove the trailing \"s\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer\n",
    "\n",
    "Previously I used HashingVectorizer to take advantage of its smaller pickle sizes for Heroku deployment. As discussed before, HashingVectorizer cannot give you backward compatibility: you lose what word corresponds to what index. Since I now want to know what the words are, I switched over to CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvbi = CountVectorizer(tokenizer=LemmaTokenizer(),ngram_range=(2,2),stop_words=nltk.corpus.stopwords.words('english'))\n",
    "bi = cvbi.fit_transform(df_rest['text'])\n",
    "cvmono = CountVectorizer(tokenizer=LemmaTokenizer(),ngram_range=(1,1),stop_words=nltk.corpus.stopwords.words('english'))\n",
    "mono = cvmono.fit_transform(df_rest['text'])\n",
    "bi_keys = cvbi.vocabulary_.keys() #[key for key in cvbi.vocabulary_.keys() if not re.match('.*[0-9_-].*',key)]\n",
    "mono_keys = cvmono.vocabulary_.keys() #[key for key in cvmono.vocabulary_.keys() if not re.match('.*[0-9_-].*',key)]\n",
    "bi_keys_split = [re.split('\\s',key) for key in bi_keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian smoothing function\n",
    "\n",
    "I tried several different approaches for setting the alpha factor (as can be seen in the commented out lines). In general, an alpha set around the mean count of all words appeared to be roughly appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bayesian_smooth(vocab,keys,data,alpha_factor=1):\n",
    "    N = data.sum()#float(sum([data[:,vocab[key]].sum() for key in keys]))\n",
    "    d = float(len(keys))\n",
    "    #alpha = float(alpha)\n",
    "    count = np.array(data.sum(0))[0]\n",
    "    print np.mean(count)\n",
    "    #alpha = np.mean(count)*float(alpha_factor)\n",
    "    bayes = {}\n",
    "    for key in keys:\n",
    "        bayes[key] = float((count[vocab[key]]+alpha_factor))\n",
    "    \n",
    "    return bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "monoalpha = 61\n",
    "bialpha = 0\n",
    "mono_vocab_smooth = bayesian_smooth(cvmono.vocabulary_,cvmono.vocabulary_.keys(),mono,monoalpha)\n",
    "bi_vocab_smooth = bayesian_smooth(cvbi.vocabulary_,cvbi.vocabulary_.keys(),bi,bialpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First results\n",
    "\n",
    "I calculated p_w and built the results into a dataframe for analysis and selection of the top 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_w = [bi_vocab_smooth[b]/(mono_vocab_smooth[s[0]]*mono_vocab_smooth[s[1]]) for b,s in zip(bi_keys,bi_keys_split)]\n",
    "dfq4 = pd.DataFrame({'prob w':p_w,\n",
    "        'bi keys':bi_keys,\n",
    "        'bi keys split':bi_keys_split})\n",
    "dfq4 = dfq4.sort('prob w',ascending=False)\n",
    "dfq4 = dfq4[dfq4['prob w'] != np.inf]\n",
    "print dfq4['prob w'].describe()\n",
    "dfq4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly more in depth\n",
    "\n",
    "Where does a phrase such as \"huevos rancheros\" appear in our list? I picked out the top phrase, huevos rancheros, and the 100th phrase to put them side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = dfq4.set_index('bi keys')\n",
    "top100 = list(dfq4['bi keys'][:100])\n",
    "#print type(top100)\n",
    "print x.xs(top100[0])\n",
    "print x.xs('huevos rancheros')\n",
    "print x.xs(top100[-1])\n",
    "top100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! (mostly)\n",
    "\n",
    "I definitely see key word pairs (mostly for various ethnic foods, which is not surprising) including the alluring \"spam musubi\" which apparently is some horrible spam based 7/11 food that Hawaiians love. Who knew?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pickle dump from joblib is then a model ready for deployment on Heroku (where it beat the benchmark for a bag of words model).\n",
    "\n",
    "Let's see what our model would predict for the first review!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1pipe.predict([review['text'],])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first quick pass could be improved in a number of ways. I used the built in \"l2\" normalization of HashingVectorizer, but alternatively I could try an TF-IDF (term frequency - inverse document frequency) normalization scheme to control for common words.\n",
    "\n",
    "I can use cross validation with GridSearchCV to optimize the hyperparameters of the RidgeCV linear regressor or just test out the performance of other linear regression estimators. Alternatively, I could test out non-linear regression algorithms, but remember we want something small for deployment on Heroku!\n",
    "\n",
    "Let's try stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.743399252329\n"
     ]
    }
   ],
   "source": [
    "sgd = linear_model.SGDRegressor(alpha=0.0001, l1_ratio=0.15, eta0=0.01, power_t=0.25)\n",
    "sgd.fit(hvcounts_train,stars_train)\n",
    "score = sgd.score(hvcounts_test,stars_test)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-583a10822ede>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'alpha'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGDRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhvcounts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stars'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\btq\\Anaconda\\lib\\site-packages\\sklearn\\grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m         \"\"\"\n\u001b[1;32m--> 732\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\btq\\Anaconda\\lib\\site-packages\\sklearn\\grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[0;32m    503\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m                                     error_score=self.error_score)\n\u001b[1;32m--> 505\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    506\u001b[0m                 for train, test in cv)\n\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\btq\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"all\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\btq\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[1;34m(self, func, args, kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \"\"\"\n\u001b[0;32m    405\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\btq\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, args, kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\btq\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[0;32m   1457\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1458\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1459\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1461\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\btq\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m    980\u001b[0m                          \u001b[0mcoef_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoef_init\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m                          \u001b[0mintercept_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mintercept_init\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 982\u001b[1;33m                          sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\btq\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m    949\u001b[0m         return self._partial_fit(X, y, alpha, C, loss, learning_rate,\n\u001b[0;32m    950\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m                                  coef_init, intercept_init)\n\u001b[0m\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m     def fit(self, X, y, coef_init=None, intercept_init=None,\n",
      "\u001b[1;32mC:\\Users\\btq\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.pyc\u001b[0m in \u001b[0;36m_partial_fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, n_iter, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m         self._fit_regressor(X, y, alpha, C, loss, learning_rate,\n\u001b[1;32m--> 900\u001b[1;33m                             sample_weight, n_iter)\n\u001b[0m\u001b[0;32m    901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\btq\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.pyc\u001b[0m in \u001b[0;36m_fit_regressor\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, sample_weight, n_iter)\u001b[0m\n\u001b[0;32m   1082\u001b[0m                           \u001b[0mlearning_rate_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m                           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meta0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1084\u001b[1;33m                           intercept_decay)\n\u001b[0m\u001b[0;32m   1085\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv = cross_validation.KFold(len(df['stars']), n_folds=10, shuffle=True)\n",
    "params = {'alpha':np.logspace(-6,-3,10)}\n",
    "grid = grid_search.GridSearchCV(linear_model.SGDRegressor(),cv=cv,param_grid=params)\n",
    "grid.fit(hvcounts,df['stars'])\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first pass, a quickly optimized SGDregressor is outperformed by RidgeCV without hyperparameter tuning!\n",
    "Bigrams\n",
    "\n",
    "My first bag of words model only considered single words (monograms) but HashingVectorizer can easily accomodate word pairings. Is there predictive power in pairs of words on the number of stars in the yelp review?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hvl2 = HashingVectorizer(norm='l2',ngram_range=(2, 2),stop_words=nltk.corpus.stopwords.words('english'))\n",
    "hvcounts = hvl2.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hvcounts_sp_train, hvcounts_sp_test, stars_train,stars_test = cross_validation.train_test_split(hvcounts,df['stars'],test_size=0.2)\n",
    "ridge_sp = linear_model.Ridge()\n",
    "ridge_sp.fit(hvcounts_sp_train,stars_train)\n",
    "score = ridge_sp.score(hvcounts_sp_test,stars_test)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_sp = linear_model.Ridge(alpha=4.)\n",
    "ridge_sp.fit(hvcounts_sp_train,stars_train)\n",
    "score = ridge_sp.score(hvcounts_sp_test,stars_test)\n",
    "#joblib.dump(ridge,'/home/vagrant/miniprojects/questions/nlp-q3ridge.pkl')\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_sp = linear_model.Ridge(alpha=2.)\n",
    "ridge_sp.fit(hvcounts_sp_train,stars_train)\n",
    "score = ridge_sp.score(hvcounts_sp_test,stars_test)\n",
    "#joblib.dump(ridge,'/home/vagrant/miniprojects/questions/nlp-q3ridge.pkl')\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q2pipe = Pipeline([\n",
    "    ('hv',hvl2),\n",
    "    ('ridge',ridge_sp)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works, but...\n",
    "\n",
    "Not as well as monograms. I next combined the monogram and bigram predictions to get a better on the whole prediction of yelp reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_train, text_test, stars_train, stars_test = cross_validation.train_test_split(df['text'],df['stars'],test_size=0.2)\n",
    "pred_q1_test = q1pipe.predict(text_test)\n",
    "pred_q2_test = q2pipe.predict(text_test)\n",
    "pred_test = [[p1,p2] for p1,p2 in zip(pred_q1_test,pred_q2_test)]\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(pred,stars_train)\n",
    "score = lm.score(pred_test,stars_test)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(lm,'/home/vagrant/miniprojects/questions/nlp-q3final.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
