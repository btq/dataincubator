{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview\n",
    "\n",
    "Unstructured data makes up the vast majority of data. This is a basic intro to handling unstructured data. Our objective is to be able to extract the sentiment (positive or negative) from review text. We will do this from Yelp review data.\n",
    "\n",
    "Your model will be assessed based on how root mean squared error of the number of stars you predict. There is a reference solution (which should not be too hard to beat). The reference solution has a score of 1.\n",
    "\n",
    "Download the data here : http://thedataincubator.s3.amazonaws.com/coursedata/mldata/yelp_train_academic_dataset_review.json.gz\n",
    "Download and parse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import gzip\n",
    "import nltk\n",
    "import simplejson\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import nltk.tokenize as tokenize\n",
    "from sklearn import linear_model\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import cross_validation, grid_search\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "\n",
    "data = gzip.open('yelp_train_academic_dataset_review.json.gz')\n",
    "data_content = data.read()\n",
    "data.close()\n",
    "lines= re.split('\\n',data_content)\n",
    "json_data = [simplejson.loads(line) for line in lines[:-1]]\n",
    "df = pd.DataFrame(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will first build a bag of words model. My strategy will be to build a linear model based on the count of the words in each document (review).\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "I first need to tokenize the yelp reviews to break them into individual words. For this process I used the `nltk` package and the `HashingVectorizer` from sklearn. `HashingVectorizer` uses a hashtable to more efficiently store the large dictionary from the yelp reviews. For testing (and scoring against a reference) I am deploying all of my models to heroku, so I essentially wanted to keep things as small as possible. The heroku box only has 500 Mb ram and is easily overwhelmed. Without this limitation a `CountVectorizer` could employed.\n",
    "\n",
    "An additional limitation of the `HashingVectorizer` is that is does not record the mapping: if you want to go back and forth that information is simply lost. But here I am concerned with a BagOfWords model to predict rating stars. I don't care what words predict 5 stars: `HashingVectorizer` should be perfectly good (and small!)\n",
    "\n",
    "Common english language words were filtered using a nltk subpackage and the stopwords command in HashingVectorizer. This removes words like \"the\", \"and\", etc. which shouldn't have any particular relevence to good or bad reviews.\n",
    "\n",
    "### Model Fitting\n",
    "\n",
    "I used a `RidgeCV` linear regression to fit my bag of words model to the number of review stars on Yelp. Cross validation was used to validate the test set and prevent overfitting.\n",
    "\n",
    "The `Pipeline` function of sklearn was used to quickly and easily link preprocessing and estimator steps of my machine learning model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hv = HashingVectorizer(norm='l2',stop_words=nltk.corpus.stopwords.words('english'))\n",
    "hvcounts = hv.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hvcounts_train, hvcounts_test, stars_train,stars_test = cross_validation.train_test_split(hvcounts,df['stars'],test_size=0.2,random_state=23)\n",
    "ridge = linear_model.RidgeCV()\n",
    "ridge.fit(hvcounts_train,stars_train)\n",
    "score = ridge.score(hvcounts_test,stars_test)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1pipe = Pipeline([\n",
    "  ('bagofwords', hv),\n",
    "  ('ridge', ridge)\n",
    "])\n",
    "joblib.dump(pipeline,'/home/vagrant/miniprojects/questions/nlp-q1pipe.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pickle dump from joblib is then a model ready for deployment on Heroku (where it beat the benchmark for a bag of words model).\n",
    "\n",
    "Let's see what our model would predict for the first review!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1pipe.predict([review['text'],])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first quick pass could be improved in a number of ways. I used the built in \"l2\" normalization of HashingVectorizer, but alternatively I could try an TF-IDF (term frequency - inverse document frequency) normalization scheme to control for common words.\n",
    "\n",
    "I can use cross validation with GridSearchCV to optimize the hyperparameters of the RidgeCV linear regressor or just test out the performance of other linear regression estimators. Alternatively, I could test out non-linear regression algorithms, but remember we want something small for deployment on Heroku!\n",
    "\n",
    "Let's try stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = linear_model.SGDRegressor(alpha=0.0001, l1_ratio=0.15, eta0=0.01, power_t=0.25)\n",
    "sgd.fit(hvcounts_train,stars_train)\n",
    "score = sgd.score(hvcounts_test,stars_test)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = cross_validation.KFold(len(df['stars']), n_folds=10, shuffle=True)\n",
    "params = {'alpha':np.logspace(-6,-3,10)}\n",
    "grid = grid_search.GridSearchCV(linear_model.SGDRegressor(),cv=cv,param_grid=params)\n",
    "grid.fit(hvcounts,df['stars'])\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first pass, a quickly optimized SGDregressor is outperformed by RidgeCV without hyperparameter tuning!\n",
    "Bigrams\n",
    "\n",
    "My first bag of words model only considered single words (monograms) but HashingVectorizer can easily accomodate word pairings. Is there predictive power in pairs of words on the number of stars in the yelp review?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hvl2 = HashingVectorizer(norm='l2',ngram_range=(2, 2),stop_words=nltk.corpus.stopwords.words('english'))\n",
    "hvcounts = hvl2.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hvcounts_sp_train, hvcounts_sp_test, stars_train,stars_test = cross_validation.train_test_split(hvcounts,df['stars'],test_size=0.2)\n",
    "ridge_sp = linear_model.Ridge()\n",
    "ridge_sp.fit(hvcounts_sp_train,stars_train)\n",
    "score = ridge_sp.score(hvcounts_sp_test,stars_test)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_sp = linear_model.Ridge(alpha=4.)\n",
    "ridge_sp.fit(hvcounts_sp_train,stars_train)\n",
    "score = ridge_sp.score(hvcounts_sp_test,stars_test)\n",
    "#joblib.dump(ridge,'/home/vagrant/miniprojects/questions/nlp-q3ridge.pkl')\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_sp = linear_model.Ridge(alpha=2.)\n",
    "ridge_sp.fit(hvcounts_sp_train,stars_train)\n",
    "score = ridge_sp.score(hvcounts_sp_test,stars_test)\n",
    "#joblib.dump(ridge,'/home/vagrant/miniprojects/questions/nlp-q3ridge.pkl')\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q2pipe = Pipeline([\n",
    "    ('hv',hvl2),\n",
    "    ('ridge',ridge_sp)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works, but...\n",
    "\n",
    "Not as well as monograms. I next combined the monogram and bigram predictions to get a better on the whole prediction of yelp reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_train, text_test, stars_train, stars_test = cross_validation.train_test_split(df['text'],df['stars'],test_size=0.2)\n",
    "pred_q1_test = q1pipe.predict(text_test)\n",
    "pred_q2_test = q2pipe.predict(text_test)\n",
    "pred_test = [[p1,p2] for p1,p2 in zip(pred_q1_test,pred_q2_test)]\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(pred,stars_train)\n",
    "score = lm.score(pred_test,stars_test)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(lm,'/home/vagrant/miniprojects/questions/nlp-q3final.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple linear regression combining a model trained on monograms and a model trained on bigrams has a stronger predictive power than either alone!\n",
    "\n",
    "This combined model beat the benchmark for a bigram model on the Heroku app.\n",
    "Top restaurant bigrams\n",
    "\n",
    "Looking at only reviews of restaurants I next wanted to identify word pairs in reviews that are more likely than the individual words alone. These might be strongly indicative of \"foodie\" type words that you might expect to find in a yelp review such as \"huevos rancheros\".\n",
    "\n",
    "We can find word pairs that are unlikely to occur consecutively based on the underlying probability of their words.\n",
    "\n",
    "Mathematically, if p(w) be the probability of a word w and p(w1w2) is the probability of the bigram w1w2, then we want to look at word pairs w1w2 where the statistic\n",
    "\n",
    "p(w1w2)/p(w1)/p(w2)\n",
    "\n",
    "is high.\n",
    "\n",
    "This metric is, however, problematic when p(w_1) and/or p(w_2) are small. This can be fixed with Bayesian smoothing or additive smoothing which essentially adds a constant factor to all probabilities. This factor sets the scale for the number of appearances a word must be used in the overall corpus before it is considered relevent.\n",
    "\n",
    "First I need to load in a second data set that idenitifies which businesses are restaurants and do an SQL style join on my two pandas dataframes. This will allow me to select reviews that only correspond to restaurants and by extension special food bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = gzip.open('/home/vagrant/miniprojects/questions/yelp_train_academic_dataset_business.json.gz')\n",
    "data_content_biz = data.read()\n",
    "data.close()\n",
    "lines= re.split('\\n',data_content_biz)\n",
    "json_data = [simplejson.loads(line) for line in lines[:-1]]\n",
    "dfbiz = pd.DataFrame(json_data)\n",
    "\n",
    "restaurant = []\n",
    "for i in dfbiz.index:\n",
    "    restaurant.append(sum([1 for cat in dfbiz.iloc[i]['categories'] if re.match('Restaurants',cat)]))\n",
    "dfbiz['restaurant'] = restaurant\n",
    "df_big = pd.merge(df,dfbiz,on='business_id')\n",
    "df_rest = df_big[df_big['restaurant']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also found it necessary at this stage to consider word lemmatization. Lemmatization is an NLP strategy to lower the vocabulary space by combining words that have the same root. For example, lemmatization should catch the plural form of a word and remove the trailing \"s\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer\n",
    "\n",
    "Previously I used HashingVectorizer to take advantage of its smaller pickle sizes for Heroku deployment. As discussed before, HashingVectorizer cannot give you backward compatibility: you lose what word corresponds to what index. Since I now want to know what the words are, I switched over to CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvbi = CountVectorizer(tokenizer=LemmaTokenizer(),ngram_range=(2,2),stop_words=nltk.corpus.stopwords.words('english'))\n",
    "bi = cvbi.fit_transform(df_rest['text'])\n",
    "cvmono = CountVectorizer(tokenizer=LemmaTokenizer(),ngram_range=(1,1),stop_words=nltk.corpus.stopwords.words('english'))\n",
    "mono = cvmono.fit_transform(df_rest['text'])\n",
    "bi_keys = cvbi.vocabulary_.keys() #[key for key in cvbi.vocabulary_.keys() if not re.match('.*[0-9_-].*',key)]\n",
    "mono_keys = cvmono.vocabulary_.keys() #[key for key in cvmono.vocabulary_.keys() if not re.match('.*[0-9_-].*',key)]\n",
    "bi_keys_split = [re.split('\\s',key) for key in bi_keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian smoothing function\n",
    "\n",
    "I tried several different approaches for setting the alpha factor (as can be seen in the commented out lines). In general, an alpha set around the mean count of all words appeared to be roughly appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bayesian_smooth(vocab,keys,data,alpha_factor=1):\n",
    "    N = data.sum()#float(sum([data[:,vocab[key]].sum() for key in keys]))\n",
    "    d = float(len(keys))\n",
    "    #alpha = float(alpha)\n",
    "    count = np.array(data.sum(0))[0]\n",
    "    print np.mean(count)\n",
    "    #alpha = np.mean(count)*float(alpha_factor)\n",
    "    bayes = {}\n",
    "    for key in keys:\n",
    "        bayes[key] = float((count[vocab[key]]+alpha_factor))\n",
    "    \n",
    "    return bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "monoalpha = 61\n",
    "bialpha = 0\n",
    "mono_vocab_smooth = bayesian_smooth(cvmono.vocabulary_,cvmono.vocabulary_.keys(),mono,monoalpha)\n",
    "bi_vocab_smooth = bayesian_smooth(cvbi.vocabulary_,cvbi.vocabulary_.keys(),bi,bialpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First results\n",
    "\n",
    "I calculated p_w and built the results into a dataframe for analysis and selection of the top 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_w = [bi_vocab_smooth[b]/(mono_vocab_smooth[s[0]]*mono_vocab_smooth[s[1]]) for b,s in zip(bi_keys,bi_keys_split)]\n",
    "dfq4 = pd.DataFrame({'prob w':p_w,\n",
    "        'bi keys':bi_keys,\n",
    "        'bi keys split':bi_keys_split})\n",
    "dfq4 = dfq4.sort('prob w',ascending=False)\n",
    "dfq4 = dfq4[dfq4['prob w'] != np.inf]\n",
    "print dfq4['prob w'].describe()\n",
    "dfq4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly more in depth\n",
    "\n",
    "Where does a phrase such as \"huevos rancheros\" appear in our list? I picked out the top phrase, huevos rancheros, and the 100th phrase to put them side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = dfq4.set_index('bi keys')\n",
    "top100 = list(dfq4['bi keys'][:100])\n",
    "#print type(top100)\n",
    "print x.xs(top100[0])\n",
    "print x.xs('huevos rancheros')\n",
    "print x.xs(top100[-1])\n",
    "top100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! (mostly)\n",
    "\n",
    "I definitely see key word pairs (mostly for various ethnic foods, which is not surprising) including the alluring \"spam musubi\" which apparently is some horrible spam based 7/11 food that Hawaiians love. Who knew?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
